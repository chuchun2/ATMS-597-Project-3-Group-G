# -*- coding: utf-8 -*-
"""Copy of Project3_getdates.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/kevingray92/ATMS-597-Project-3-Group-G/blob/master/Project3_getdates.ipynb

# ATMS 597: Project 3
"""

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline
import xarray as xr
from datetime import datetime
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import requests
import matplotlib.pyplot as plt

!pip install netcdf4
!pip install pydap

years = np.arange(1996,2020)

file_list = []
associatedyearlist = []

# Get the file names from the website by looping through each year
for year in years:
    page = requests.get('https://www.ncei.noaa.gov/data/global-precipitation-climatology-project-gpcp-daily/access/'+str(year)+'/')
    soup = BeautifulSoup(page.text, 'html.parser')

    file_name_list_items = soup.find_all('a')
    file_name_list_items

    i = 0
    for file_name in file_name_list_items:
        # Need an if statement to skip some header lines on each page
        if i > 4:
            names = file_name.contents[0]
            #print(names)
            file_list.append(names)
            associatedyearlist.append(year)
        i = i+1
# We now have a list of file names and their associated years

# Check the length of the lists to make sure we got all that we wanted
len(file_list)
#len(associatedyearlist)

# Test getting just one file
!wget -nv "https://www.ncei.noaa.gov/data/global-precipitation-climatology-project-gpcp-daily/access/2018/gpcp_v01r03_daily_d20180101_c20180409.nc"
nc = xr.open_dataset('gpcp_v01r03_daily_d20180101_c20180409.nc')

# Check the file contents if you want
nc['precip']

# This is the loop to get all the data.

site = 'https://www.ncei.noaa.gov/data/global-precipitation-climatology-project-gpcp-daily/access/'
datasets = [] # an empty list that will hold all the precip data arrays
for count in range(len(file_list)):
    file_i, year_i = (file_list[count], associatedyearlist[count])
    fileloc = (site+str(year_i)+'/'+file_i)
    !wget -nv "{fileloc}" # Get the file
    nc = xr.open_dataset(file_i) # read the file into nc
    precip = xr.DataArray(nc['precip'], dims=['time', 'latitude', 'longitude']) # load the precip data into a data array 
    datasets.append(precip) #append the datasets list with the precip data array
    !rm "{file_i}" # remove the downloaded file
len(datasets)
combined = xr.concat(datasets, dim='time') # Make one combined file with all the data arrays

# Check the length of datasets to make sure we got everything
len(datasets)

# Save the combined file
combined.to_netcdf('combined.nc')

# or mount Google Drive and then save it there
from google.colab import drive
drive.mount('/content/drive')

# Save the combined file on Google Drive
combined.to_netcdf('/content/drive/My Drive/combined.nc')

# Plot the data
preciptest = combined.mean('time')
preciptest.plot()

"""From here on, the 'combined.nc' file can be loaded and then we can begin getting the dates that had rain exceeding the 95th percentile."""

allprecip = xr.open_dataset('/content/drive/My Drive/combined.nc')

# Only get the months we want. Oct, Nov, and Dec
def is_ond(month):
    return (month >= 10) & (month <= 12)

precip_OND = allprecip.sel(time=is_ond(allprecip['time.month']))

# Get the precip values at Kinshasa, Congo lat = -4.4, lon = 15.3
precipvalues = precip_OND['precip'].sel(latitude=-4.4, longitude=15.3, method='nearest').values

# Put precip values in a list for quick plotting and checking for unrealistic data
precipvalueslist = precipvalues.tolist()

#precipvalueslist.sort(reverse=True) # Uncomment to check for unrealistic large values
#print(precipvalueslist)
#precipvalueslist.index(9.969209968386869e+36) # Find the index of the unrealistic value
precipvalueslist[1963] = 0.0 # replace unrealistic data with 0.0

# Plot a quick histogram
plt.hist(precipvalueslist, bins = 20)
plt.show()

# Get data in a 1x1 degree grid over Kinshasa, Congo
kinshasaraw = precip_OND.sel(latitude=slice(-4.4-1., -4.4+1.), longitude=slice(15.3-1, 15.3+1))
kinshasaraw

# Or just get data at the closest grid point to Kinshasa, Congo
kinshasaloc = precip_OND.sel(latitude=-4.4, longitude=15.3, method='nearest')
kinshasalocclean = kinshasaloc.where(kinshasaloc.precip<=8000.0, drop=True) # only get values less than 8000 mm/day to get rid of unrealistic data

# Calculate the 95th percentile
kinshasap95 = kinshasaraw.quantile(0.95, dim='time')
p95value = average(kinshasap95.precip)

# An alternate way to get the 95th percentile
np.percentile(kinshasaraw.precip.values, 95)

# Plot the Cumulative Distribution Function
n_bins = 100

# set up some dummy data for drawing the 95th percentile line
x = np.arange(0.0, 75.0, 5.0)
y = np.arange(0.0, 75.0, 5.0)
y[:] = 0.95 # y is set to 0.95 at all x so that we get a straight line when plotting x, y

fig, ax = plt.subplots(figsize=(8, 4))

# plot the cumulative histogram
n, bins, patches = ax.hist(kinshasalocclean.precip.values, n_bins, density=True, histtype='step',
                           cumulative=True, label='Empirical', color='blue')
# plot a line at the 95th percentile
line = ax.plot(x, y, 'k--')

# set up the title and axis labels
plt.title('Cumulative Distribution Function of Rainfall in Oct, Nov, and Dec \n and 95th Percentile')
plt.xlabel('Precipitation bins (mm/day)')
plt.ylabel('Percentile')

# Make a new dataset with only dates exceeding the 95th percentile and also lower than 8000.0 mm to filter out bad data
datesexceeded = kinshasaraw.where((kinshasaraw.precip>=p95value) & (kinshasaraw.precip<=8000.0), drop=True)

# Check the max precip value to make sure unrealistic values were removed properly.
datesexceeded.precip.max()

datesexceeded

datesexceeded.time.values

# Save the datesexceeded data array to Google Drive. This will be input for the next portion of code.
datesexceeded.to_netcdf('/content/drive/My Drive/datesexceeded.nc')

"""From here on, the 'datesexceededclean.nc' file can be loaded and then we can begin plotting the composite fields."""